**Business Problem:**\
Many people struggle to get loans due to poor or non-existent credit histories. Unfortunately, in search of loans, this population is often taken advantage of by untrustworthy lenders. Home Credit aims to help the unbanked population by providing a better borrowing experience. In order to make sure this population has a positive loan experience, Home Credit makes use of a variety of data to predict their clients' ability to repay.

**Project Objective:**
While Home Credit is currently using various statistical and machine learning methods to make these predictions, our objective is to help them unlock the full potential of their data. Doing so will increase the likelihood that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their success.

**Group's Solution:**
Our solution to the problem is a Gradient Boosting model to help predict each client's likelihood of repayment. Our gradient boosting model was the best performing model with an AUC of .92 through cross validation and a .65 Kaggle score. Our gradient boosting model performed the best for a few reasons. The first is that it sequentially trains which means unlike random forest's independent tree construction, gradient boosting trains trees sequentially. The next variable that helped was its effective regularization which can mitigate overfitting in a model. Our data is highly complex so this is extremely valuable compared to other models. Lastly is that our data is highly imbalanced and gradient boosting models have the ability to assign higher weights to imbalanced data.

**Individual Contribution:**
My contribution included some of our EDA. We all performed our own individual EDA analysis prior, but upon switching the project to Python, my contribution on the EDA was lower. I contributed to a majority of the visualizations in the EDA however which are entailed in the Kaggle Individual Contributions file. I mainly contributed in the model building and interpretation. I largely contributed to building our random forest and gradient boosting models as well as the code that interpretted the most significant features in the gradient boosting and the following interpretation.

**Business Value:**
Our solution contributes back to the business by being able to gleam individual features that help predict a client's ability to repay a loan. The first important feature for Home Credit to be aware of is "Owning a car". Whether the client does or doesn't own a car was a highly favored feature in the gradient boosting model. The next feature was the external credit scores received. While we understand that Home Credit's goal is to help clients with poor or non-existent credit histories, credit scores still need some value attached to them as they are a strong indicator of repayability. The final two features which showed themselves as important were lenghth of employment and ownership of other real estate. With this information, Home Credit can increase their prediction abilities on who will and won't repay their loans. Further, we performed a cost-benefit analysis on the our models and Home Credit needs to decide if they would prefer the longer-computing more-accurate model or the shorter-computing less-accurate model.

**Technical Difficulties:**
The first and largest technical difficulty we faced was an improper output of predictions which greatly halted our progress. Rather than our model outputting probability predictions it was outputting all 0's and 1's as if it was rounding to majority classes instead of the specific probabilities. This was solved by changing the models to specifically predict the probability of being a 0 or 1 which allowed our results to print as we liked. The next difficulty was a worst data preparation. Our data was largely imbalanced and through our attempted downsampling we don't think we fixed that imbalanced problem  as much as we would have liked for a more accurate model. Lastly was the large amount of missing data. We opted to input values into most of the missing data but some columns were missing upwards of 80% of their data and we would've been better off removing those as we didn't find value from columns with so much inputted data. If anything it may have harmed our results.

**What Was Learned:**
I learned a great deal in this project. The first was the importance of a proper data preparation when using real (out of classroom) data. I was very accustomed to classroom data where you need to perform minimal data preparation for it to be ready. This data however needed much more and caused us to perform preparations, attempt models, and then have to go back to preparation multiple times after receiving improper results. I learned the importance of time management here as well. My group struggled to find time in all of our conflicting schedules and while we achieved it, we weren't as on-time as we would have liked and I think it's important before even beginning to layout a proper schedule to meet so that you are on top of it from the very beginning. Lastly, I learned that there is a lot of trial and error involved in finding the right model. It wasn't until our 5th model attempt where we saw improvements and were able to get to a model that performed well enough for the goal of the project. Planning time around restarting and trying again is very crucial in this process.
